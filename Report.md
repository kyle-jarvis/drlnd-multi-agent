# Udacity DRLND
## P3 Multi agent / collaboration

### Environment

Two agents take actions in a tennis-style environment. Each agent controls the movement
of one 'raquet'. Each agent receives its own perspective of the environment.
As described in [here](./UdacityP3MultiAgent.md), agents receive a reward of -0.01
if the ball lands on the ground in their half of the court. Agents receive a reward
of +0.1 for 'batting' the ball over the net. Therefore, agents receive a high reward
by establishing volleys collaboratively. 

## Learning Algorithm

The learning algorithm used to collect the below results is the Multi-Agent Deep 
Deterministic Policy Gradients algorithm described [here](https://arxiv.org/pdf/1706.02275.pdf).
Each agent _i_ posses policy networks μ<sub>_i_</sub>, μ'<sub>_i_</sub>, and
critic networks _Q_<sub>_i_</sub>, _Q_'<sub>_i_</sub>. Critic networks are 'central'
in that they receive information from all agents' perspectives, as well as all
agents' actions. Policy networks act only on information local to the agent to 
which they belong. 

This allows for stable learning of policies that allow agents to behave collaboratively
or competitatively whilst only requiring local observations at run-time.


For the results illustrated below, the architecture was as follows:

A. Each agent is identical in its perception of the representation of the environment
and the action space that it can explore. Each agent has its own set of networks 
that are identical in their architecture.

B. Policy networks
1. An input layer of (33) neurons.
2. A first hidden layer of 128 neurons.
3. A second hidden layer of 128 neurons.
4. An output layer of 2 neurons.

C. Critic
1. An input layer of (33*2) + (4*2) neurons.
2. A first hidden layer of 128 neurons.
3. A second hidden layer of 128 neurons.
4. An output layer of 1 neurons.

All layers are fully-connected, with ReLU activations applied to the neurons, except
for the output layers, where the Actor network has a tanh activation applied and
the critic has a simple linear activation.

Training proceeds in the usual way. During training, noise can be added to the agents
action selection. The noise scale is tapered linearly over the course of training.
The output layers of the actor and critic networks are intialised with very small 
weights so that the noise is allowed to intially dominate the action selection,
and the critic outputs are initially small compared to the environment rewards.

The other layers in the actor/critic networks have their weights and biases intialised according to the Glorot/Xavier-Uniform [method](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_uniform_).

### Hyperparameters

1. Learning rate : 5e-4

2. Tau (soft update parameter) : 1e-3

3. Minibatch size : 64

4. Memory buffer size : 5e5

5. Gamma : 0.99

6. Noise distribution (normal):
    - σ : 1.0 → 0.0 over 250 episodes.
    - μ : 0.0

7. Noise distribution ([Ornstein Uhlenbeck](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process)):
    - θ : 0.15
    - σ : 1.0 → 0.0 over 250 episodes.
    - μ : 0.0
    - dt : 0.01

## Results

We have investigated how the learning process is affected by the particular
distribution of noise that is added to the agent's actions in order to define
the exploration policy. We have recorded how the agent learns when there is:

1. A noise generating process where noise is sampled from a normal distribution. The noise at each time step within one episode is identical and independent. The width of the normal distribution is linearly tapered to 0 over 250 episodes.
2. A noise generating process where noise is sampled from an Ornstein Uhlenbeck process. The noise generated by this process if autocorrelated, which helps in simulations of physical systems where inertial is present. The region explored by the noise generating process is tapered to 0 over 250 episodes.

Interestingly, despite the OU process being touted as an efficient exploration policy in these types of environments, we find the agent learns quicker when the noise generating process is uncorrelated.

This is probably not a limitation in this environment, because the target area is in motion, very often intersecting with the arm to generate a reward, and so even with an inefficient exploration policy (in terms of physical space explored) there is strong discrimination between states that can be learned from.

<img src = "./resources/comparison.png" width="300"/>

The GIF below shows a set of articulated limbs being controlled by the agent trained with the Normal noise generating process in the image above.

<img src = "./resources/reacher.gif" width="300" height=180/>

## Future work

The code included in this repository can be extended in order to investigate
various aspects of reinforcement learning in this simple setting. 

1) Given that the noise distribution employed in the exploration policy bears a significant impact on the learning curves, the exploration of the parameters that govern this noise could be a source of further gains in the learning rate and ultimate performance.
2) A dynamic mini-batch size, which increases over time, might be beneficial to fine-tuning the agent in the later part of the training, where the agent receives the same reward most of the time. At this stage most transitions will yield the same reward, and so the likelihood of there being useful information in a given minibatch of transitions decreases.